{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576bcb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "120/120 [==============================] - 69s 549ms/step - loss: 1.1927 - acc: 0.2199 - val_loss: 1.0886 - val_acc: 0.2602\n",
      "Epoch 2/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 1.0258 - acc: 0.2768 - val_loss: 1.0033 - val_acc: 0.3010\n",
      "Epoch 3/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.9356 - acc: 0.3170 - val_loss: 0.9326 - val_acc: 0.3312\n",
      "Epoch 4/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 0.8536 - acc: 0.3565 - val_loss: 0.8575 - val_acc: 0.3756\n",
      "Epoch 5/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.7760 - acc: 0.3964 - val_loss: 0.8014 - val_acc: 0.4067\n",
      "Epoch 6/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.7088 - acc: 0.4333 - val_loss: 0.7549 - val_acc: 0.4377\n",
      "Epoch 7/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.6495 - acc: 0.4687 - val_loss: 0.7191 - val_acc: 0.4633\n",
      "Epoch 8/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.5968 - acc: 0.4990 - val_loss: 0.6736 - val_acc: 0.4897\n",
      "Epoch 9/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.5496 - acc: 0.5284 - val_loss: 0.6457 - val_acc: 0.5099\n",
      "Epoch 10/40\n",
      "120/120 [==============================] - 65s 542ms/step - loss: 0.5069 - acc: 0.5581 - val_loss: 0.6225 - val_acc: 0.5294\n",
      "Epoch 11/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.4687 - acc: 0.5852 - val_loss: 0.5998 - val_acc: 0.5437\n",
      "Epoch 12/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.4336 - acc: 0.6100 - val_loss: 0.5800 - val_acc: 0.5541\n",
      "Epoch 13/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.4017 - acc: 0.6333 - val_loss: 0.5648 - val_acc: 0.5658\n",
      "Epoch 14/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 0.3728 - acc: 0.6550 - val_loss: 0.5529 - val_acc: 0.5746\n",
      "Epoch 15/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.3469 - acc: 0.6759 - val_loss: 0.5422 - val_acc: 0.5843\n",
      "Epoch 16/40\n",
      "120/120 [==============================] - 65s 541ms/step - loss: 0.3218 - acc: 0.6941 - val_loss: 0.5299 - val_acc: 0.5897\n",
      "Epoch 17/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.2991 - acc: 0.7142 - val_loss: 0.5197 - val_acc: 0.5947\n",
      "Epoch 18/40\n",
      "120/120 [==============================] - 65s 541ms/step - loss: 0.2788 - acc: 0.7313 - val_loss: 0.5125 - val_acc: 0.6036\n",
      "Epoch 19/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.2600 - acc: 0.7472 - val_loss: 0.5056 - val_acc: 0.6066\n",
      "Epoch 20/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.2420 - acc: 0.7631 - val_loss: 0.5026 - val_acc: 0.6113\n",
      "Epoch 21/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.2262 - acc: 0.7766 - val_loss: 0.4967 - val_acc: 0.6116\n",
      "Epoch 22/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 0.2112 - acc: 0.7902 - val_loss: 0.4916 - val_acc: 0.6164\n",
      "Epoch 23/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 0.1975 - acc: 0.8025 - val_loss: 0.4879 - val_acc: 0.6188\n",
      "Epoch 24/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.1845 - acc: 0.8140 - val_loss: 0.4843 - val_acc: 0.6229\n",
      "Epoch 25/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.1724 - acc: 0.8250 - val_loss: 0.4824 - val_acc: 0.6241\n",
      "Epoch 26/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.1617 - acc: 0.8342 - val_loss: 0.4803 - val_acc: 0.6261\n",
      "Epoch 27/40\n",
      "120/120 [==============================] - 65s 542ms/step - loss: 0.1514 - acc: 0.8437 - val_loss: 0.4824 - val_acc: 0.6267\n",
      "Epoch 28/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.1419 - acc: 0.8530 - val_loss: 0.4802 - val_acc: 0.6275\n",
      "Epoch 29/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.1331 - acc: 0.8613 - val_loss: 0.4824 - val_acc: 0.6263\n",
      "Epoch 30/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.1248 - acc: 0.8703 - val_loss: 0.4792 - val_acc: 0.6276\n",
      "Epoch 31/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.1178 - acc: 0.8761 - val_loss: 0.4787 - val_acc: 0.6292\n",
      "Epoch 32/40\n",
      "120/120 [==============================] - 65s 542ms/step - loss: 0.1108 - acc: 0.8835 - val_loss: 0.4803 - val_acc: 0.6308\n",
      "Epoch 33/40\n",
      "120/120 [==============================] - 65s 541ms/step - loss: 0.1042 - acc: 0.8899 - val_loss: 0.4793 - val_acc: 0.6340\n",
      "Epoch 34/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.0983 - acc: 0.8953 - val_loss: 0.4803 - val_acc: 0.6331\n",
      "Epoch 35/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.0925 - acc: 0.9009 - val_loss: 0.4821 - val_acc: 0.6335\n",
      "Epoch 36/40\n",
      "120/120 [==============================] - 65s 539ms/step - loss: 0.0875 - acc: 0.9058 - val_loss: 0.4830 - val_acc: 0.6349\n",
      "Epoch 37/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.0827 - acc: 0.9105 - val_loss: 0.4854 - val_acc: 0.6342\n",
      "Epoch 38/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 0.0781 - acc: 0.9147 - val_loss: 0.4869 - val_acc: 0.6325\n",
      "Epoch 39/40\n",
      "120/120 [==============================] - 65s 540ms/step - loss: 0.0737 - acc: 0.9187 - val_loss: 0.4907 - val_acc: 0.6324\n",
      "Epoch 40/40\n",
      "120/120 [==============================] - 65s 538ms/step - loss: 0.0696 - acc: 0.9226 - val_loss: 0.4923 - val_acc: 0.6302\n",
      "Prediction on the training dataset\n",
      "\n",
      "Input Source sentence: your ticket please\n",
      "Actual Target Translation:  το εισιτήριό σας παρακαλώ \n",
      "Predicted Target Translation:  το εισιτήριό σας παρακαλώ \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# As sample dataset ell-eng.txt (Greek-English) was used https://www.manythings.org/anki/ell-eng.zip\n",
    "\n",
    "filename = 'ell.txt'\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "latent_dim=256\n",
    "\n",
    "# open the file eng-fin.txt and read\n",
    "lines = pd.read_table(filename,  names =['source', 'target', 'comments'])\n",
    "\n",
    "\n",
    "# convert source and target text to Lowercase \n",
    "lines.source = lines.source.apply(lambda x: x.lower())\n",
    "lines.target = lines.target.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes from source and target text\n",
    "lines.source = lines.source.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.target = lines.target.apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "# create a set of all special characters\n",
    "special_characters= set(string.punctuation)\n",
    "\n",
    "# Remove all the special characters\n",
    "lines.source = lines.source.apply(lambda x: ''.join(ch for ch in x if ch not in special_characters))\n",
    "lines.target = lines.target.apply(lambda x: ''.join(ch for ch in x if ch not in special_characters))\n",
    "\n",
    "# Remove digits from source and target sentences\n",
    "num_digits= str.maketrans('','', string.digits)\n",
    "lines.source = lines.source.apply(lambda x: x.translate(num_digits))\n",
    "lines.target = lines.target.apply(lambda x: x.translate(num_digits))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.source = lines.source.apply(lambda x: x.strip())\n",
    "lines.target = lines.target.apply(lambda x: x.strip())\n",
    "lines.source = lines.source.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.target = lines.target.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines.target = lines.target.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "\n",
    "# Find all the source and target words and sort them\n",
    "# Vocabulary of Source language\n",
    "all_source_words=set()\n",
    "for source in lines.source:\n",
    "    for word in source.split():\n",
    "        if word not in all_source_words:\n",
    "            all_source_words.add(word)\n",
    "\n",
    "# Vocabulary of Target \n",
    "all_target_words=set()\n",
    "for target in lines.target:\n",
    "    for word in target.split():\n",
    "        if word not in all_target_words:\n",
    "            all_target_words.add(word)\n",
    "            \n",
    "# sort all unique source and target words\n",
    "source_words = sorted(list(all_source_words))\n",
    "target_words = sorted(list(all_target_words))\n",
    "\n",
    "#Find maximum sentence length in  the source and target data\n",
    "source_length_list, target_length_list= [], []\n",
    "for l in lines.source:\n",
    "    source_length_list.append(len(l.split(' ')))\n",
    "max_source_length= max(source_length_list)\n",
    "\n",
    "for l in lines.target:\n",
    "    target_length_list.append(len(l.split(' ')))\n",
    "max_target_length= max(target_length_list)\n",
    "\n",
    "# creating a word to index(word2idx) for source and target\n",
    "source_word2idx= dict([(word, i+1) for i,word in enumerate(source_words)])\n",
    "target_word2idx=dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "\n",
    "#creating a dictionary for index to word for source and target vocabulary\n",
    "source_idx2word= dict([(i, word) for word, i in  source_word2idx.items()])\n",
    "target_idx2word =dict([(i, word) for word, i in target_word2idx.items()])\n",
    "\n",
    "#Shuffle the data\n",
    "lines = shuffle(lines)\n",
    "\n",
    "# Train - Test Split\n",
    "X, y = lines.source, lines.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Input tokens for encoder\n",
    "num_encoder_tokens=len(source_words)\n",
    "# Input tokens for decoder zero padded\n",
    "num_decoder_tokens=len(target_words) + 1\n",
    "\n",
    "\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_source_length),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_target_length),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_target_length, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = source_word2idx[word] \n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_word2idx[word]\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, target_word2idx[word]] = 1.\n",
    "                    \n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "            \n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens+1, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Set up decoder to return full output sequences\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that takes encoder and decoder input to output decoder_outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(generate_batch(),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)\n",
    "\n",
    "# Encode the input sequence to get the \"Context vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of \n",
    "    #target sequence with the start character.\n",
    "    target_seq[0, 0] = target_word2idx['START_']\n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)# Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word =target_idx2word[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word# Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True# Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index# Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=0\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Prediction on the training dataset' + '\\n')\n",
    "print('Input Source sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Target Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Target Translation:', decoded_sentence[:-4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5ed8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e75c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
